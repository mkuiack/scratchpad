{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "import json\n",
    "import tkp.db\n",
    "import tkp.config\n",
    "import logging\n",
    "import csv\n",
    "import time\n",
    "import sys\n",
    "import itertools\n",
    "import pylab\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import healpy as hp\n",
    "import datetime\n",
    "import os\n",
    "import glob\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import SkyCoord, match_coordinates_sky\n",
    "from astropy.io import fits\n",
    "from astropy.wcs import WCS\n",
    "\n",
    "\n",
    "import numbers\n",
    "import math\n",
    "import scipy\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import sem\n",
    "from scipy import linspace\n",
    "from scipy import pi,sqrt,exp\n",
    "from scipy.special import erf\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy import asarray as ar,exp\n",
    "from scipy import interpolate, signal\n",
    "\n",
    "\n",
    "import pymc3 as pm\n",
    "from scipy.stats import norm\n",
    "\n",
    "from scipy import linspace\n",
    "from scipy import pi,sqrt,exp\n",
    "from scipy.special import erf\n",
    "\n",
    "from matplotlib.ticker import NullFormatter\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "plt.rcParams['font.size']=16\n",
    "plt.rcParams['axes.labelsize']='large'\n",
    "plt.rcParams['axes.titlesize']='large'\n",
    "pylab.rcParams['legend.loc'] = 'best'\n",
    "matplotlib.rcParams['text.usetex'] = False\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nsf(num, n=1):\n",
    "    \"\"\"n-Significant Figures\"\"\"\n",
    "    numstr = (\"{0:.%ie}\" % (n-1)).format(num)\n",
    "    return float(numstr)\n",
    "\n",
    "def num_err(num, err, n=1):\n",
    "    '''Return number rounded based on error'''\n",
    "    return np.around(num,int(-(np.floor(np.log10(nsf(err,n=n)))))), nsf(err,n=n)\n",
    "\n",
    "def clip(data, sigma=3):\n",
    "    \"\"\"Remove all values above a threshold from the array.\n",
    "    Uses iterative clipping at sigma value until nothing more is getting clipped.\n",
    "    Args:\n",
    "        data: a numpy array\n",
    "    \"\"\"\n",
    "    data = data[np.isfinite(data)]\n",
    "    raveled = data.ravel()\n",
    "    median = np.median(raveled)\n",
    "    std = np.nanstd(raveled)\n",
    "    newdata = raveled[np.abs(raveled-median) <= sigma*std]\n",
    "    if len(newdata) and len(newdata) != len(raveled):\n",
    "        return clip(newdata, sigma)\n",
    "    else:\n",
    "        return newdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf(x):\n",
    "    return 1/sqrt(2*pi) * exp(-x**2/2)\n",
    "\n",
    "def cdf(x):\n",
    "    return (1 + erf(x/sqrt(2))) / 2\n",
    "\n",
    "def skewnorm(x,e=0,w=1,a=0):\n",
    "    t = (x-e) / w\n",
    "    return 2 / w * pdf(t) * cdf(a*t)\n",
    "\n",
    "\n",
    "def delta(shape):\n",
    "    return (shape/pm.math.sqrt(1.0+shape**2.))\n",
    "\n",
    "\n",
    "def muz(shape):\n",
    "    return pm.math.sqrt(2./np.pi)*delta(shape) \n",
    "\n",
    "def skewness(shape):\n",
    "    return (4.- np.pi)/2. * ((delta(shape)*pm.math.sqrt(2./np.pi))**3.)/(1.0-(2.0*delta(shape)**2.)/np.pi)**(3./2.)\n",
    "\n",
    "def sigmaz(shape):\n",
    "    return np.sqrt(1.-muz(shape)**2.)\n",
    "\n",
    "\n",
    "def skew_mode(shape):\n",
    "    return pm.math.sqrt(2.0/np.pi)*delta(shape) - \\\n",
    "        skewness(shape) * pm.math.sqrt(1.0 - (pm.math.sqrt(2.0/np.pi)*delta(shape))**2 )/2.0 - \\\n",
    "        (pm.math.sgn(shape) / 2.0) *( pm.math.exp (-(2.0*np.pi)/pm.math.abs_(shape)))\n",
    "\n",
    "\n",
    "def sk_mode(loc,scale,shape):\n",
    "    return loc + skew_mode(shape) * scale\n",
    "\n",
    "def fit_lightcurve(y, draws=500):\n",
    "    with pm.Model() as model:\n",
    "\n",
    "        (mu, sigma) = norm.fit(y)\n",
    "\n",
    "        loc = pm.Normal(\"loc\",mu, 20)\n",
    "        scale = pm.HalfNormal(\"scale\", sigma)\n",
    "        skew = pm.Normal(\"skew\", 0, 5)\n",
    "        mode = pm.Deterministic(\"mode\",sk_mode(loc,scale,skew))\n",
    "        _y = pm.SkewNormal(\"y_dist\",mu=loc, sd=scale,alpha=skew, observed=y)\n",
    "        trace = pm.sample(draws=draws)\n",
    "    mode = pm.summary(trace)[pm.summary(trace).index == \"mode\"][\"mean\"].values[0]\n",
    "    mode_err = pm.summary(trace)[pm.summary(trace).index == \"mode\"][\"sd\"].values[0]\n",
    "        \n",
    "    return mode, mode_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_trans(dbname, dataset_id, engine, host, port, user, pword):\n",
    "    tkp.db.Database(\n",
    "        database=dbname, user=user, password=pword,\n",
    "        engine=engine, host=host, port=port\n",
    "    )\n",
    "\n",
    "    # find all the new, candidate transient, sources detected by the pipeline\n",
    "    transients_query = \"\"\"\n",
    "    SELECT  tr.runcat\n",
    "           ,tr.newsource_type\n",
    "           ,im.rms_min\n",
    "           ,im.rms_max\n",
    "           ,im.detection_thresh\n",
    "           ,ex.f_int\n",
    "    FROM newsource tr\n",
    "         ,image im\n",
    "         ,extractedsource ex\n",
    "    WHERE tr.previous_limits_image = im.id\n",
    "      AND tr.trigger_xtrsrc = ex.id\n",
    "    \"\"\"\n",
    "    \n",
    "    cursor = tkp.db.execute(transients_query, (dataset_id,))\n",
    "    transients = tkp.db.generic.get_db_rows_as_dicts(cursor)\n",
    "    print \"Found\", len(transients), \"new sources\"\n",
    "    return transients\n",
    "\n",
    "def dump_sources(dbname, dataset_id, engine, host, port, user, pword):\n",
    "    tkp.db.Database(\n",
    "        database=dbname, user=user, password=pword,\n",
    "        engine=engine, host=host, port=port\n",
    "    )\n",
    "    # extract the properties and variability parameters for all the running catalogue sources in the dataset\n",
    "    sources_query = \"\"\"\\\n",
    "    SELECT  im.taustart_ts\n",
    "            ,im.tau_time\n",
    "            ,ex.f_int\n",
    "            ,ex.f_int_err\n",
    "            ,ex.f_peak\n",
    "            ,ex.f_peak_err\n",
    "            ,ax.xtrsrc\n",
    "            ,ex.extract_type\n",
    "            ,ex.det_sigma\n",
    "            ,ax.runcat as runcatid\n",
    "            ,ex.ra\n",
    "            ,ex.decl\n",
    "            ,ex.ra_err\n",
    "            ,ex.decl_err\n",
    "            ,im.band\n",
    "            ,im.rms_min\n",
    "            ,im.rms_max\n",
    "            ,ax.v_int\n",
    "            ,ax.eta_int\n",
    "            ,ax.f_datapoints\n",
    "            ,im.freq_eff\n",
    "            ,im.url\n",
    "    FROM extractedsource ex\n",
    "         ,assocxtrsource ax\n",
    "         ,image im\n",
    "         ,runningcatalog rc\n",
    "    WHERE ax.runcat = rc.id\n",
    "      AND ax.xtrsrc = ex.id\n",
    "      and ex.image = im.id\n",
    "      AND rc.dataset = %s\n",
    "      ORDER BY rc.id\n",
    "    \"\"\"\n",
    "    cursor = tkp.db.execute(sources_query, (dataset_id,))\n",
    "    sources = tkp.db.generic.get_db_rows_as_dicts(cursor)\n",
    "\n",
    "    print \"Found\", len(sources), \"source datapoints\"\n",
    "\n",
    "    return sources "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distSquared(p0, p1):\n",
    "    distance  = np.sqrt((p0[0] - p1[0,:])**2 + (p0[1] - p1[1,:])**2)\n",
    "    if np.min(distance) < 3.0:\n",
    "        return np.where(distance == np.min(distance))[0]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def pol2cart(rho, phi):\n",
    "    x = rho * np.cos(phi)\n",
    "    y = rho * np.sin(phi)\n",
    "    return(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_source(full_data, run_id):\n",
    "    source_df = full_data[(full_data.runcatid == run_id)]\n",
    "    \n",
    "#     source_df = source_df.groupby('taustart_ts', as_index=False)\n",
    "    source_df.set_index(source_df.taustart_ts, inplace=True)\n",
    "    return source_df.sort_index()\n",
    "\n",
    "\n",
    "def plot_lightcurve(full_data, run_id, ion_sub=False, roll_len = 1*60, roll_type = 'triang', stdout=True):\n",
    "\n",
    "    source_df = full_data[(full_data.runcatid == run_id)]\n",
    "    if stdout:\n",
    "        print source_df.wm_ra.iloc[0], source_df.wm_decl.iloc[0]\n",
    "\n",
    "    pd.to_datetime(source_df.taustart_ts)\n",
    "    source_df = source_df.groupby('taustart_ts', as_index=False).mean()\n",
    "    source_df.set_index(source_df.taustart_ts, inplace=True)\n",
    "\n",
    "    if ion_sub:\n",
    "        rolling = source_df.f_int.rolling(roll_len, win_type=roll_type)\n",
    "        source_df.f_int = source_df.f_int-rolling.mean()\n",
    "\n",
    "\n",
    "    plt.rcParams['font.size']=16\n",
    "    plt.rcParams['axes.labelsize']='large'\n",
    "    plt.rcParams['axes.titlesize']='large'\n",
    "    pylab.rcParams['legend.loc'] = 'best'\n",
    "\n",
    "    ylim = [np.nanmean(source_df.f_int)-6.0*np.nanstd(source_df.f_int),\n",
    "            np.nanmean(source_df.f_int)+10.0*np.nanstd(source_df.f_int)]\n",
    "\n",
    "\n",
    "\n",
    "    myFmt = mdates.DateFormatter('%H:%M')\n",
    "    source_df[\"taustart_ts\"] = pd.to_datetime(source_df[\"taustart_ts\"])\n",
    "    obs_dates = np.unique([100*x.month+x.day for x in source_df[\"taustart_ts\"]])\n",
    "\n",
    "    n_hours = np.array([]) \n",
    "    for i in obs_dates:\n",
    "        index = (100*pd.DatetimeIndex(source_df[\"taustart_ts\"]).month+pd.DatetimeIndex(source_df[\"taustart_ts\"]).day == i)# & (source_df.extract_type == 0)\n",
    "        n_hours = np.append(n_hours, len(np.unique(pd.DatetimeIndex(source_df[\"taustart_ts\"][index]).hour)))\n",
    "    hour_ratio = [i/n_hours.sum() for i in n_hours ]\n",
    "    gs_ratio = np.append((hour_ratio)/min(hour_ratio),1)\n",
    "\n",
    "    gs = gridspec.GridSpec(1, len(obs_dates)+1, width_ratios=gs_ratio) \n",
    "\n",
    "    figcount = 0\n",
    "    figure = plt.figure(figsize=(4*len(obs_dates),6))\n",
    "\n",
    "    for i in obs_dates:\n",
    "        index = (100*pd.DatetimeIndex(source_df[\"taustart_ts\"]).month+pd.DatetimeIndex(source_df[\"taustart_ts\"]).day == i)# & (source_df.extract_type == 0)\n",
    "        ax = plt.subplot(gs[figcount])\n",
    "        ax.locator_params(nticks=6)\n",
    "        ax.errorbar(source_df[\"taustart_ts\"].values[index],\n",
    "                    source_df[\"f_int\"].values[index],\n",
    "                    yerr=source_df[\"f_int_err\"].values[index],\n",
    "                    fmt=\".\",c=\"#1f77b4\",ecolor=\"#ff7f0e\")\n",
    "\n",
    "        if figcount > 0:\n",
    "            ax.set_yticks([])\n",
    "        if figcount ==0:\n",
    "            plt.ylabel(\"Flux [Jy]\")\n",
    "            ax.yaxis.set_ticks_position('left')\n",
    "        if stdout:\n",
    "            print source_df[\"taustart_ts\"].values[index][0]\n",
    "        plt.annotate(\"{}-{}\".format(pd.DatetimeIndex(source_df[\"taustart_ts\"].values[index]).day[0],\n",
    "                                    pd.DatetimeIndex(source_df[\"taustart_ts\"].values[index]).month[0]),\n",
    "                                    xy=(0.95,0.95), xycoords='axes fraction',\n",
    "                                    horizontalalignment='right', verticalalignment='top',fontsize=16)\n",
    "\n",
    "        plt.xticks(rotation=90)\n",
    "        ax.set_ylim(ylim)\n",
    "        ax.xaxis.set_major_formatter(myFmt)\n",
    "        figcount+=1\n",
    "\n",
    "    hist_index = np.isfinite(source_df[\"f_int\"]) #& (source_df.extract_type == 0)\n",
    "    plt.subplot(gs[figcount])\n",
    "    (mu, sigma) = norm.fit(source_df[\"f_int\"].iloc[hist_index].values)\n",
    "    n, bins, patches   =  plt.hist(source_df[\"f_int\"].values[hist_index],\n",
    "                                   bins=100,normed=1, orientation='horizontal',facecolor=\"#1f77b4\")\n",
    "    y = mlab.normpdf( bins, mu, sigma)\n",
    "    if stdout:\n",
    "        print \"Gaus fit: mu {}, sigma {}\".format(round(mu,3),round(sigma,3))\n",
    "    \n",
    "    l = plt.plot(y,bins,'r--', linewidth=2)\n",
    "    # plt.title(\"Source: N = {}\".format(len(source_df[\"f_int\"].values[np.isfinite(source_df[\"f_int\"])])))\n",
    "    plt.annotate(\"Total N:\\n{}\".format(len(source_df[\"f_int\"].values[hist_index])),\n",
    "                                xy=(0.95,0.95), xycoords='axes fraction',\n",
    "                                horizontalalignment='right', verticalalignment='top',fontsize=16)\n",
    "    # plt.ylabel(\"Normalized N\")\n",
    "    plt.yticks([])\n",
    "    plt.ylim(ylim)\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0)\n",
    "    return figure\n",
    "#     plt.show()\n",
    "# fig.text(0.5, 0.04, 'date', ha='center')\n",
    "# plt.tight_layout()\n",
    "# print(source_df[\"wm_ra\"].values[0],source_df[\"wm_decl\"].values[1])\n",
    "# plt.savefig(\"{}_multiday_lightcurve.png\".format(key))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import database username/password/etc.\n",
    "sys.path.append('/home/kuiack')\n",
    "from database_info import *\n",
    "query_loglevel = logging.WARNING  # Set to INFO to see queries, otherwise WARNING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_stats = pd.read_csv(\"/home/kuiack/survey_stats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbname = sys.argv[1]\n",
    "outfile = \"/data/AS\"+dbname+\"_Candidates/AARTFAAC_cat_flux2.csv\"\n",
    "ObsDir = \"/data/AS\"+dbname+\"_Candidates/\"\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "if not os.path.exists(ObsDir):\n",
    "    os.makedirs(ObsDir)\n",
    "\n",
    "dataset = survey_stats[survey_stats.obs == dbname].dataset.values[0]\n",
    "timesteps = survey_stats[survey_stats.obs == dbname].timestamp.values[0]\n",
    "\n",
    "print \"database name: \",  dbname\n",
    "logging.getLogger('sqlalchemy.engine').setLevel(query_loglevel)\n",
    "\n",
    "db = tkp.db.Database(engine=engine, host=host, port=port,\n",
    "                     user=user, password=password, database=dbname)\n",
    "\n",
    "db.connect()\n",
    "session = db.Session()\n",
    "sources = dump_sources(dbname, dataset, engine, host, port, user, password)\n",
    "print len(sources)\n",
    "data = pd.DataFrame(sources)\n",
    "\n",
    "del sources\n",
    "\n",
    "data.taustart_ts = pd.to_datetime(data.taustart_ts)\n",
    "\n",
    "# Remove bad source fits \n",
    "data = data.drop(data.index[np.abs(data.f_int) > 10e6])\n",
    "# No observations are greater than 24 hours \n",
    "data = data[data.taustart_ts.diff() < datetime.timedelta(seconds=24*3600)]\n",
    "\n",
    "# try:\n",
    "#     data[\"round_times\"] = [datetime.datetime.strptime(os.path.basename(x)[:19], \n",
    "#                                                       \"%Y-%m-%dT%H:%M:%S\") for x in data.url.values]\n",
    "# except ValueError:\n",
    "#     data[\"round_times\"] = [datetime.datetime.strptime(os.path.basename(x)[:14], \n",
    "#                                                       \"%Y%m%d%H%M%S\") for x in data.url.values]\n",
    "\n",
    "# survey_stats.set_value(survey_stats.obs == dbname,\"timestamp\", len(np.unique(data.round_times)))\n",
    "# survey_stats.to_csv(\"survey_stats.csv\", index=False)\n",
    "\n",
    "db._configured = False\n",
    "del db, session\n",
    "\n",
    "print time.time() - t1, \"seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection in both subbands, simultaneously, in > N_detections timesteps. \n",
    "\n",
    "# N_detections = 0 \n",
    "\n",
    "\n",
    "# # multi_detections = [] \n",
    "# reduced = pd.DataFrame([])\n",
    "\n",
    "\n",
    "# t1 = time.time()\n",
    "# for _id in np.unique(data.runcatid):\n",
    "#     if len(data[(data.runcatid == _id) & \\\n",
    "#                 (data.band == 23) & \\\n",
    "#                 (data.extract_type == 0)].set_index(\"round_times\").index.\\\n",
    "#            intersection(data[(data.runcatid == _id) & \\\n",
    "#                              (data.band == 24) & \\\n",
    "#                              (data.extract_type == 0)].set_index(\"round_times\").index )) > N_detections \\\n",
    "#     and (np.max(data[(data.runcatid == _id )].det_sigma) > 8 ):\n",
    "\n",
    "#         if len(reduced) == 0:\n",
    "#             reduced = pd.DataFrame(data[(data.runcatid == _id)])\n",
    "#         else:\n",
    "#             reduced = pd.concat([reduced,data[(data.runcatid == _id)]])\n",
    "\n",
    "# print time.time() - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = data.groupby(\"runcatid\").median()\n",
    "# base[\"taustart_ts\"] = data.groupby(\"runcatid\").first().taustart_ts\n",
    "base[\"f_datapoints\"] = data.groupby(\"runcatid\").last().f_datapoints\n",
    "# base[\"timestep\"] = [x.timestamp() for x in base.taustart_ts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vlssr = pd.read_csv(\"/home/kuiack/VLSSr_gt_5.csv\", comment=\"#\")\n",
    "# tgss = pd.read_csv(\"/home/kuiack/TGSSADR1_7sigma_catalog.tsv\", delimiter=\"\\t\")\n",
    "aart = pd.read_csv(\"/home/kuiack/AARTFAAC_catalogue.csv\")\n",
    "# ateam = {\"ra\":np.array([82.88,299.43,350.28,187.07]),\n",
    "#          \"decl\":np.array([21.98,40.59,58.54,12.66])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aart_coord = SkyCoord(aart.ra.values*u.deg, aart.decl.values*u.deg, frame='fk5')\n",
    "# ateam_coord = SkyCoord(ateam[\"ra\"]*u.deg, ateam[\"decl\"]*u.deg, frame='fk5')\n",
    "AART_catsource = pd.DataFrame([], columns=base.keys())\n",
    "\n",
    "for i in base[base.f_datapoints > 1800].index:\n",
    "    try:\n",
    "        c1 = SkyCoord(base.loc[i].ra*u.deg, base.loc[i].decl*u.deg, frame='fk5')\n",
    "\n",
    "        c2 = SkyCoord(base.drop(index=i).ra.values*u.deg, \n",
    "                  base.drop(index=i).decl.values*u.deg, frame='fk5')\n",
    "    except IndexError:\n",
    "        print i\n",
    "\n",
    "    if np.min(c1.separation(aart_coord).deg) < 1:\n",
    "        if len(AART_catsource) == 0:\n",
    "            AART_catsource = pd.DataFrame(base.loc[i]).T\n",
    "        else:\n",
    "            AART_catsource = pd.concat([AART_catsource, pd.DataFrame(base.loc[i]).T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(outfile):\n",
    "    fields=['ra','decl','mode_lo','mode_lo_err','mode_hi','mode_hi_err','N']\n",
    "    with open(outfile, 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(fields)\n",
    "\n",
    "for _ID in AART_catsource.index:\n",
    "\n",
    "    mode_lo,mode_lo_err = fit_lightcurve(clip(data[(data.band == 23) & \\\n",
    "                                              (data.runcatid == _ID) & \\\n",
    "                                              (data.extract_type == 0)].f_int.values), draws=100)\n",
    "\n",
    "    mode_hi,mode_hi_err = fit_lightcurve(clip(data[(data.band == 24) & \\\n",
    "                                              (data.runcatid == _ID) & \\\n",
    "                                              (data.extract_type == 0)].f_int.values), draws=100)\n",
    "\n",
    "    N = len(data[(data.band == 24) & (data.runcatid == _ID)].f_int.values)\n",
    "    ra = np.nanmean(data[(data.band == 24) & (data.runcatid == _ID)].ra.values)\n",
    "    decl = np.nanmean(data[(data.band == 24) & (data.runcatid == _ID)].decl.values)\n",
    "\n",
    "    with open(outfile, 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([ra,decl,mode_lo,mode_lo_err,mode_hi,mode_hi_err,N])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
